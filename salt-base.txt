#saltstack#

----------

##简介及安装
- 三大功能
	- 远程执行
	- 配置管理(状态，不可回滚)
	- 云管理
- 官网
	- 用户组 [http://www.saltstack.cn/](http://www.saltstack.cn/)
	- 正式官网 [http://www.saltstack.com/](http://www.saltstack.com/)  
- 四种运行方式
	- Local
	- Minion / Master
	- syndic(代理模式)
	- salt ssh
- 官方仓库
	- [http://repo.saltstack.com/](http://repo.saltstack.com/)
- 安装
	- yum安装repo仓
	<pre>yum install https://repo.saltstack.com/yum/redhat/salt-repo-latest-1.el7.noarch.rpm -y</pre>
	- salt-master的安装
	<pre>yum install salt-master -y</pre>
	- 客户端安装
	<pre>yum install salt-minion -y</pre>
	- 其他
	<pre>yum install salt-ssh -y
	yum install salt-syndic -y
	yum install salt-cloud -y
	yum install salt-api -y
	</pre>
- salt-master服务配置
	- salt-master服务启动
	`systemctl start salt-master.service`
- salt-minion配置
	- 配置文件 `vi /etc/salt/minion`
	- 修改内容
	<pre>1. #master salt 修改为master masterIP(推荐在主机名可解析的情况下用hostname)
	2. #id: 修改为id： 自定义为IP#添加minion的唯一标识符，变动频繁用IP，比较固定用hostname
	3. systemctl start salt-minion.service</pre>
##远程执行##
- 认证
	- 1. salt-minion启动后，在`/etc/salt/pki/minion`目录下产生公钥和私钥文件:`minion.pem` 和 `minion.pub`
	- 2. 在salt-master的 `/etc/salt/minions_pre`会接收minion端主机的私钥文件： `linux-node2` ，暂时存放这里
	- 3. 在salt-master执行 `salt-key -a linux*`认证后， `/etc/salt/minions_pre`下的私钥文件 `linux-node2`会被保存到`/etc/salt/minions`下;同时salt-minion端的`/etc/salt/pki/minion`会保存salt-master的公钥文件：`minion_master.pub`。至此，认证完成。  
- salt命令
	- salt '*' test.ping
	- salt '*' cmd.run 'Commd'
	- salt '*' state.sls 

##配置管理##
- state模块说明
	- 是以YAML语法编写、后缀名为`.sls`的文件
- YAML基础：三个规则
	- 规则1
		>缩进：2个空格，不能使用`tab`
	- 规则2
		>冒号：  
			a. 和空格配合表示层级关系  
			b. 表示键/值对，eg: `my_key: my_value`
	
	- 规则3
		>\- ：列表。eg：
			<pre>- list_value_one
			- list_value_two
			- list_value_three</pre>
- salt-master配置文件示例
	- 修改文件保存位置
<pre>file_roots:
  base:
    - /srv/salt/
  dev:
    - /srv/salt/dev/services
    - /srv/salt/dev/states
  prod:
    - /srv/salt/prod/services
    - /srv/salt/prod/states
</pre>
>说明:base名字是不能修改的，dev和prod的名字随便改

- 示例：安装apache
	- 文本
	<pre># cat /srv/salt/web/apace.sls 
	apache-install:
	  pkg.installed:
	    - names:
	      - httpd
	      - httpd-devel
	
	apache-service:
	  service.running:
	    - name: httpd
	    - enable: True</pre>
 
	- 执行安装
	`salt 'linux-node2' state.sls web.apace`
	>在salt-master执行上述命令后，apace.sls文件会被发送到minion的`/var/cache/salt/minion/files/base/web/apace.sls`下被minion执行。
- stat.highstat高级state设置示例
	>默认保存在base根环境下  

	- 脚本
	<pre>vi top.sls
	base:
	  'linux-node2':
	    - web.apace
	  'manage':
	    - web.apace
	</pre>
	- 执行命令  
	`salt 'linux-node2' state.highstate test=True` # 先检测正确性，正确的情况下再执行,目标不要带`*`  
	`salt 'linux-node2' state.highstate `  
	**top file的作用是给指定的目标给特定的状态**。
##ZeroMQ与saltstack##
- ZeroMQ用到的2种模式
	- 发布与订阅模式（pub and sub）。salt-master的4505端口用于发送信息。
	- 请求和响应模式。salt-master的4506端口用于接收minion端消息
		
		>查看salt进程名： `yum install  -y python-setproctitle`
##saltstack数据系统##
两种数据系统
	- Grains （谷粒）
	- Pillar	 （柱子）
###Grains###
它是一种静态数据，是minion在启动的时候收集的minion本地的相关信息。如操作系统版本，内核版本等。  

- 作用  
	1.  资产管理，信息查询
	2.  用于目标选择
		<pre>salt -G 'os:Centos' test.ping</pre>
		 # 使用大G做目标选择
	3.  配置管理中使用
		<pre>salt 'cobbler' grains.ls
		salt 'linux-node2.oldboy.com' grains.items
		salt 'linux-node2.oldboy.com' grains.item fqdn_ip4
		salt 'linux-node2.oldboy.com' grains.item os</pre>
- 自定义
	1. 在minion里边自定义grains
		<pre>grains:
		  roles: testname</pre>
	 重启minion服务后测试:
		<pre>salt 'linux-node2.oldboy.com' grains.item roles    
		linux-node2.oldboy.com:
    	    ----------
    	roles:
            apache</pre>
	示例：执行echo命令
		<pre>salt -G 'roles:apache' cmd.run 'echo hehe' 
		linux-node2.oldboy.com:
    	    hehe</pre>
\# 生产中建议将grains配置写在 `/etc/salt/grains`文件中，写入以K/V格式,如 `testroles: testv`
- salt刷新grains命令  
	`salt '*' saltutil.sync_grains`
- grains在top文件使用示例：
	<pre>base:
	  'linux-node2':
	    - web.apace
	  'roles: apace'
	    - match: grain
	    - web.apace</pre>

- 配置管理案例
- grains开发使用示例
	- 创建grains目录和测试文件
	<pre>mkdir /srv/salt/_grains
	vi my_grains.py
	#!/usr/bin/env python
	#_*_ coding: utf-8 _*_

	def my_grains():
    #初始化一个grains字典
    grains = {}
    #设置值
    grains['iaas'] = 'openstack'
    grains['edu'] = 'oldboyedy'
    #返回这个字典
    return grains
	</pre>
	- 同步测试文件
	<pre>salt '*' saltutil.sync_grains 
	linux-node2:
	    - grains.my_grains
	manage:
	    - grains.my_grains</pre>
	
	文件会被保存在minion端的`/var/cache/salt/minion/files/base/_grains`目录下，而`/var/cache/salt/minion/extmods/grains/`目录下的文件是用来被执行
	- 结果
	<pre>salt '*' grains.item iaas
	linux-node2:
	    ----------
	    iaas:
	        openstack
	manage:
	    ----------
	    iaas:
	        openstack</pre>   
- grains的运行优先级
	1. 系统自带
	2. grains文件写的
	3. minion配置文件写的
	4. 自己写的
###pillar###

- 简述  
	**pillar数据是动态的，给特定的minionz指定特定的数据。只有指定的minion自己能看到自己的数据**

- 应用场景  
	用在比较对数据敏感的场合，如密码  

- pillar查看  
	`salt '*' pillar.items`

- pillar的master配置  
	`#pillar_opts: False`找到这行，去掉注释并修改值为`True`

- 自定义pillar
	<pre>vi /etc/salt/master
		#pillar_roots:
	#  base:
	#    - /srv/pillar</pre>
	去掉注释，创建base相关目录，重启`salt-master`即可
- pillar实践
	1. 建立pillar测试文件
		<pre>cat /srv/pillar/apache.sls 
		{% if grains['os'] == 'Centos' %}
		apache: httpd
		{% elif grains['os'] == 'Debian' %}
		apache: apache2
		{% endif %}</pre>

	2. top file中给特定的minion指定特定的数据
		<pre>vim /srv/pillar/top.sls
		base:
		  'linux-node2':
		    - web.apace</pre>
	3. pillar刷新命令  
		`salt '*' saltutil.refresh_pillar`

	4. 查看  
	 `salt '*' pillar.items apache`
- 使用场景
	- 目标选择
##pillar和grains的区别##

名称 | 类型 | 数据采集方式 | 应用场景 |定义位置
-----|------|------------|--------|-----
grains| 静态 |minion启动时候收集 |数据查询 目标选择 配置管理|minion
pillar| 动态 |master自定义      |目标选择 配置管理 敏感数据|master

##深入学习saltstack远程执行##
- 命令讲解  
	`salt '*' cmd.run 'w'`
	>命令：salt  
	>目标： '*'  
	>模块： cmd.run  
	>返回：执行后结果的返回，Returnners

###目标targeting###
---
**所有匹配目标的方式，都可以用到top file里边来指定目标**  

1. 和minion有关
	1. minion ID  如： 'linux-node1'
	2. 通配符   如：linux-node[1|2]'
		>列表方式： salt -L __'linux-node1,linux-node2'__ test.ping
		>正则方式： salt -E __linux-(node1|node2)__ test.ping
2. 和minion无关
	>子网、IP地址
	<pre># salt -S 10.0.0.171 test.ping 
	linux-node2.oldboy.com:
	    True
	# salt -S 10.0.0.0/24 test.ping   
	linux-node2.oldboy.com:
	    True
	cobbler:
	    True</pre>
	
##主机名设置方案##
1. IP地址
2. 根据业务来设置
	>redis-node1-redis04-idc04-soa.example.com  
	>redis-node1为第一个节点  
	>redis04为第4个集群  
	>idc04为哪个机房  
	>soa为业务线
##指定目标匹配总结##
[https://www.unixhot.com/docs/saltstack/topics/targeting/compound.html](https://www.unixhot.com/docs/saltstack/topics/targeting/compound.html)
##执行模块##

- 模块地址链接  
[https://www.unixhot.com/docs/saltstack/ref/index.html](https://www.unixhot.com/docs/saltstack/ref/index.html)
- 常用几个模块  
	 + network
	 + service
		 + available
		 + status
		 + reload  
	 - cp 
	 	>salt-cp '*' /etc/hosts /tmp/hehe
	 - state
		>state.show_top
##返回程序##
- 返回相关模块  
[https://www.unixhot.com/docs/saltstack/ref/returners/index.html](https://www.unixhot.com/docs/saltstack/ref/returners/index.html)
- 执行单独安装某个任务
	`salt '*' state.single pkg.installed name=MySQL-python`
- 以mysql为实例
	- 建立salt库及相关表
	<pre>CREATE DATABASE  `salt`
	  DEFAULT CHARACTER SET utf8
	  DEFAULT COLLATE utf8_general_ci;
	
	USE `salt`;
	
	--
	-- Table structure for table `jids`
	--
	
	DROP TABLE IF EXISTS `jids`;
	CREATE TABLE `jids` (
	  `jid` varchar(255) NOT NULL,
	  `load` mediumtext NOT NULL,
	  UNIQUE KEY `jid` (`jid`)
	) ENGINE=InnoDB DEFAULT CHARSET=utf8;
	CREATE INDEX jid ON jids(jid) USING BTREE;
	
	--
	-- Table structure for table `salt_returns`
	--
	
	DROP TABLE IF EXISTS `salt_returns`;
	CREATE TABLE `salt_returns` (
	  `fun` varchar(50) NOT NULL,
	  `jid` varchar(255) NOT NULL,
	  `return` mediumtext NOT NULL,
	  `id` varchar(255) NOT NULL,
	  `success` varchar(10) NOT NULL,
	  `full_ret` mediumtext NOT NULL,
	  `alter_time` TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
	  KEY `id` (`id`),
	  KEY `jid` (`jid`),
	  KEY `fun` (`fun`)
	) ENGINE=InnoDB DEFAULT CHARSET=utf8;
	
	--
	-- Table structure for table `salt_events`
	--
	
	DROP TABLE IF EXISTS `salt_events`;
	CREATE TABLE `salt_events` (
	`id` BIGINT NOT NULL AUTO_INCREMENT,
	`tag` varchar(255) NOT NULL,
	`data` mediumtext NOT NULL,
	`alter_time` TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
	`master_id` varchar(255) NOT NULL,
	PRIMARY KEY (`id`),
	KEY `tag` (`tag`)
	) ENGINE=InnoDB DEFAULT CHARSET=utf8;
	------
	grant all on salt.* to salt@localhost identified by 'salt';	
	</pre>
- 执行模块  
	`salt '*' test.ping --return mysql`
##编写状态模块##
1. 文件保存目录位置  
	`/srv/salt/_modules`
2. 文件命名
	文件名就是模块名。如：my_disk.py
	<pre>vi my_disk.py
	def list():
	  cmd = 'df -h'
	  ret = __salt__['cmd.run'](cmd)
	  return ret</pre>
3. 刷新执行
	<pre>salt '*' saltutil.sync_modules
	salt '*' my_disk.list</pre>
<<<<<<< HEAD
##其他组件
1. Mine
	>主要应用场景是配合前端负载均衡动态获取Mine汇报信息，来动态生成配置文件。例如官网通过mine.get指定业务设备的网卡地址动态生成haprxoy.cfg文件。Mine还支持get docker容器的地址，可简单实现动态添加业务。  
	- 两种配置放置  
		a.  是通过在Minion配置文件中定义
		b.  是通过模块的方式去下发Mine采集任务。
		<pre>salt 'Minion' mine.send network.ip_addrs Minion:
		True
		salt 'Minion' mine.get 'Minion' network.ip_Minion:
		----------
		Minion:
		- 172.17.42.1</pre>

2. Peer
=======
##job管理

__job管理的2种方法：__  

- stal-run   
- module
###salt-run管理
---
1. 查看salt-run对job管理的一些用法
	<pre>salt-run -d|grep jobs
	'jobs.active:'
    Return a report on all actively running jobs from a job id centric
        salt-run jobs.active
	'jobs.list_job:'
        salt-run jobs.list_job 20130916125524463507 #指定jid查看jobs详细信息
        salt-run jobs.list_job 20130916125524463507 --out=pprint
	'jobs.list_jobs:'
    List all detectable jobs and associated functions
	        salt-run jobs.list_jobs_filter 100 filter_find_job=False
	'jobs.lookup_jid:'
        salt-run jobs.lookup_jid 20130916125524463507
        salt-run jobs.lookup_jid 20130916125524463507 --out=highstate
	'jobs.print_job:'
        salt-run jobs.print_job 20130916125524463507
	</pre>
2. 使用模块来管理
	- 使用帮助查询
		<pre>salt \* sys.doc saltutil|grep job 
		saltutil.find_cached_job:
		    Return the data for a specific cached job id. Note this only works if
		    cache_jobs has previously been set to True on the minion.
		        salt '*' saltutil.find_cached_job <job id>
		saltutil.find_job:
		    Return the data for a specific job id that is currently running.
		        The job id to search for and return data.
		        salt '*' saltutil.find_job <job id>
		    Note that the find_job function only returns job information when the job is still running. If
		    the job is currently running, the output looks something like this:
		        # salt my-minion saltutil.find_job 20160503150049487736
		    If the job has already completed, the job cannot be found and therefore the function returns
		        # salt my-minion saltutil.find_job 20160503150049487736
		saltutil.kill_job:
		    Sends a kill signal (SIGKILL 9) to the named salt job's process
		        salt '*' saltutil.kill_job <job id>
		        salt '*' saltutil.runnerjobs.list_jobs</pre>
	- 示例
	<pre># salt 'cobbler' cmd.run 'sleep 100; who'
	^C^CExiting on Ctrl-C
	This job's jid is:
	20160917221918444177
	---
	# salt 'cobbler' saltutil.find_job 201
	cobbler:
    ----------
	</pre>
##Event和Reactor
---

- Event比job更加底层，是saltstack里边对每个时间的一个记录，记录的更详细
	- 查看Event事件
		<pre></pre>
##配置管理
1. sls文件
	>- sls文件是描述文件,是saltstack的缩写，状态不可回滚  
	>- sls状态模块文档官网地址：  
		<pre>https://docs.saltstack.com/en/latest/ref/states/all/</pre> 
	>- sls文件结构距离说明
		<pre># cat /srv/salt/web/apace.sls 
		apache-install:			#名称或ID声明，默认是name声明，如php： pkg.installed，会去安装php 
		  pkg.installed:		#状态（state）声明。这是是用指定的模块的方法去干什么事
		    - names:			#选项声明。指定要干的事情的内容
		      - httpd	
		      - httpd-devel
		apache-service:			#ID最好必须唯一，特别是高级状态
		  service.running:
		    - name: httpd
		      - enable: True</pre>

2. LAMP架构实例
	- 规划
		>a. 安装软件包    pkg  
		>b. 修改配置文件  file  
		>c. 启动服务      service
	- pkg模块常用方法
		- pkg.installed	#安装
			>pkg.installed模块示例：
			<pre>common_packages:
		 	pkg.installed:
		      - pkgs:
		        - unzip
		        - dos2unix
		        - salt-minion: 2015.8.5-1.el6</pre>	
		- pkg.latest #确保最新版本
		- pkg.remove #卸载
		- pkg.purge	#卸载并删除配置文件  
	- file模块常用方法
		- file.managed
			>backup: 对象
		- file.append
			>text 
	- service模块常用方法  
		- service.running
		- 示例说明
			<pre>redis:
		  service.running:		
		    - enable: True		#设置可以开机启动
		    - reload: True		#设置可以自动重载
		    - watch:
		      - pkg: redis</pre>
	- LAMP状态文件-lamp.sls
		<pre>lamp-pkg:
          pkg.installed:
		    - pkgs:
		      - httpd
		      - php
		      - mariadb
		      - mariadb-server
		      - php-mysql
		      - php-cli
		      - php-mbstring
		
		apache-config:		#另一种方法是以文件名为ID声明
		  file.managed:		#一个ID声明下面，状态模块不能重复出现
		    - name: /etc/http/conf/httpd.conf 
		    - source: salt://lamp/files/httpd.conf
		    - user: root
		    - group: root
		    - mode: 644
		
		php-config:
		  file.managed:
		    - name: /etc/php.ini
			- source: salt://lamp/files/php.ini
			- user: root
			- group: root
			- mode: 644
		
		mysql-config:
		  file.managed:
		    - name: /etc/my.cnf
		    - source: salt://lamp/files/my.cnf
		    - user: root
		    - group: root
		    - mode: 644
		
		apache-service:
		  service.running:
			- name: httpd
		    - enable: True
		    - reload: True
		    
		mysql-service:
		  service.running:
			- name: mariadb
			- enable: True 
			- reload: True</pre>
		__`salt://`表示当前环境的根目录，即`salt-master`的`master`配置文件中的`file_roots`下的`base`路径__ ，另一种写法：
		<pre>
	    apache-server:
		  pkg.install:
		    - pkgs:
			  - httpd
			  - php
		      - php-mysql
		      - php-cli
		      - php-mbstring
		  file.managed:
			- name: /etc/http/conf/http.conf 
		    - source: salt://lamp/files/httpd.conf
		    - user: root
		    - group: root
		    - mode: 644
		  server.running:
			- name: httpd
			- enable: True
			- reload: True
		
		mysql-server:
		  pkg.install:
		    - pkgs:
		      - mariadb
		      - mariadb-server
		  file.managed:
		    - name: /etc/my.cnf
		    - source: salt://lamp/mysql/my.cnf
		    - user: root
		    - group: root
		    - mode: 644
		  server.running:
			- name: mariadb
			- enable: True
			- reload: True
			
		php-config:
		  file.managed:
		    - name: /etc/php.ini
			- source: salt://lamp/php/php.ini
			- user: root
			- group: root
			- mode: 644
		</pre>	
		
	- 在salt-master上建立相关目录
		<pre>mkdir /srv/salt/lamp
		mkdir /srv/salt/lamp/files
		cp /etc/my.cnf /etc/php.ini /etc/httpd/conf/httpd.conf /srv/salt/lamp/files
		vi /srv/salt/lamp/lamp.sys</pre>
	- 执行单个状态文件
		<pre>salt 'linux-node2' state.sls lamp.lamp</pre>
3. 状态间依赖关系
	1. 我依赖谁： __require__  
		<pre>apache-service:
		  service.running:
		    - name: httpd
		    - enable: True
		    - reload: True
		    - require:
		      - pkg: lamp-pkg
		      - file: apache-config</pre>
	2. 我被谁依赖： __require_in__
		<pre>mysql-config:
		  file.managed:
		    - name: /etc/my.cnf
		    - source: salt://lamp/files/my.cnf
		    - user: root
		    - group: root
		    - mode: 644
		    - require_in:
		      - service: mysql-service</pre>
	3. 我监控谁： __watch__
		<pre>apache-service:
	  service.running:
	    - name: httpd
	    - enable: True
	    - reload: True
	    - require:
	      - pkg: lamp-pkg
	    - watch:
	      - file: apache-config</pre>
	   	> **watch的作用是监视`apache-config`的状态的变化，这里如果没有 `reload: True` 的话，会直接重启(`restart`)该服务**
	4. 我被谁监控： __watch_in__
	5. 我引用谁： __include__
		>模块化思维：原子化
		<pre>cat init.sls
		include:
		  - lamp.pkg
		  - lamp.config
		  - lamo.service
		</pre>
	6. 我扩展谁
	7. unless
		<pre>cmd.run a
		unless: test -L /usr/local/haproxy</pre>
		>以上test的值为真就不执行cmd.run
	8. onlyif
4. 编写sls技巧
	1. 按状态分类： 如果单独使用，很清晰，如单独的pkg、config、service的sls文件
	2. 按服务分类： 可以被其他的`sls include`,如LNMP include mysql服务
##Jinja2模板
###文档地址： [http://docs.jinkan.org/docs/jinja2/](http://docs.jinkan.org/docs/jinja2/)
- 分隔符
	1.  {% ... %} ：用于执行诸如 for 循环 或赋值的语句
	2.   {{ ... }}  ：把表达式的结果打印到模板上
- 变量的表示
	1. {{ foo.bar }}
	2. {{ foo['bar'] }}
- Jinja2在YAML中的使用
	1. 在sls状态文件中的file中使用`template： jinja`声明
	2. 然后列出参数列表`defaults: PORT：80`
	3. 模板使用
		>在配置文件中把需要修改个地址改为 `{{ PORT }}`，支持salt、grains、pillar赋值，如在httpd.conf模板中：
		`Listen： {{ grains['fqdn_ip4'][0] }}:{{ PORT }}`
		`{{ salt['network.hw_addr']('eth0') }}`
		`{{ pillar['apache'] }}`，其中pillar用于配置用户名密码的时候
	4. 写在sls里边的defaults中，变量列表中：
		<pre>- defaults:
		  IPADDR: {{ grains['fqdn_ip4'][0] }}
		  PORT: 88</pre>
##配置管理小结
- sls文件里的三个声明
	- ID  
	- 状态 
	- 选项
- 状态文件的可以按类来写
	- 按状态
	- 按服务
- 状态间的依赖关系
	- require or require_in
	- watch or watch_in
	- include or include_in
- Jinja2模板的应用
##salt项目实战
###1. 系统初始化
1. 系统初始化
2. 功能模块
	- 设置单独的目录
		- haproxy
		- nginx
		- php
		- mysql
		- memcached
3. 业务模块
	- 根据业务类型划分
		- web服务
		- 论坛bbs 
####1.1 salt环境配置

- base 基础环境
- prod 生产环境  
	`mkdir /srv/salt/{base,prod}`  
- pillar的设置    
	`mkdir /srv/pillar/{base,prod} -p`
	- 环境初始化：
		1. dns配置
			<pre>/etc/resolv.conf:
			  file.managed:
			    - source: salt://init/files/resolv.conf
			    - user: root
			    - group: root
			    - mode: 644</pre>
		2. histroy记录时间
			<pre>/etc/profile:
			  file.append:
			    - text:
			      - export HISTTIMEFORMAT="%F %T `whoami`"</pre>
			>这里是`file.append`方法，在文件的末尾追加`text：`的内容
		3. 记录命令操作
			<pre>vi audit.sls 
			/etc/bashrc:
			  file.append:
			    - text:
			      - export PROMPT_COMMAND='{ msg=$(history 1 | { read x y; echo $y; });logger
			 "[euid=$(whoami)]":$(who am i):[`pwd`]"$msg"; }'</pre>
		4. 内核参数优化
			<pre>net.ipv4.ip_local_port_range:
			  sysctl.present:
			    - value: 10000 65000
			fs.file-max:
			  sysctl.present:
			    - value: 2000000
			net.ipv4.ip_forward:
			  sysctl.present:
			    - value: 1
			vm.swappiness:
			  sysctl.present:
			- value: 0</pre>
		5. 安装yum仓库
			<pre>yum_repo_release:
			  pkg.installed:
			    - sources:
			      - epel-release: http://mirrors.aliyun.com/epel/epel-release-latest-7.noarch
			.rpm</pre>
		6. 安装zabbix-agent
			1. 基本设置
				<pre>cat salt/base/init/zabbix_agent.sls 
			zabbix-agent:
			  pkg.installed:
			    - name: zabbix-agent
			  file.managed:
			    - name: /etc/zabbix/zabbix_agentd.conf
			    - source: salt://init/files/zabbix_agentd.conf
			    - template: jinja
				- backup: minion
			    - defaults:
			      Zabbix_Server: {{ pillar['zabbix-agent']['Zabbix_Server'] }}
				  Hostname: {{ grains[fqdn] }}
			    - require:
			      - pkg: zabbix-agent
			  service.running:
			    - enable: True
			    - watch:
			      - pkg: zabbix-agent
			      - file: zabbix-agent
			zabbix_agentd.conf.d:
			  file.directory:
			    - name: /etc/zabbix/zabbix_agentd.d
			    - watch_in:
			      - service: zabbix-agent
			    - require:
			      - pkg: zabbix-agent
			    - file: zabbix-agent</pre>
			2. pillar下的base设置,即`Server: {{ pillar['zabbix-agent']['Zabbix_Server'] }}`的值
				<pre>vi pillar/base/zabbix/agent.sls 
			zabbix-agent:
			  Zabbix_Server: 10.0.0.160</pre>
			3. top文件设置
				<pre>cat /srv/pillar/base/top.sls 
				base:
				  '*':
				    - zabbix.agent</pre>
- 备份功能
	>在sls文档中的`file.managed`下加入`backup: minion`,执行后会将档案备份在`/var/cache/salt/minion/XX_backup`目录下
####1.2 haproxy
1. 创建需要的目录
	<pre>cd /srv/salt/prod;mkdir haproxy keepalived memcached nginx php pkg</pre>
2. 常用服务包的安装状态文件
	<pre>cat /srv/salt/prod/pkg/make.sls 
	make-pkg:
	  pkg.installed:
	    - pkgs:
	      - gcc
	      - gcc-c++
	      - glibc
	      - make
	      - autoconf
	      - openssl
	      - openssl-devel
	      - pcre
	      - pcre-devel</pre>
	- haproxy默认安装
		<pre>make TARGET=linux2628
		make install PREFIX=/usr/local/haproxy-1.6.3
		ln -s /usr/local/haproxy-1.6.3 /usr/local/haproxy</pre>
	- 修改启动脚本
		<pre>[root@cobbler haproxy-1.6.3]# vi examples/haproxy.init
		BIN=/usr/local/haproxy/sbin/$BASENAME</pre>
	- 复制启动文件
		<pre>cp examples/haproxy.init ../../files/</pre>	 
-	haproxy sls文件
	<pre># cd /srv/salt/prod/haproxy/
	vi install.sls
	include:
	  - pkg.make
	
	haproxy-install:
	  file.managed:
	    - name: /usr/local/src/haproxy-1.6.3.tar.gz
	    - source: salt://haproxy/files/haproxy-1.6.3.tar.gz
	    - mode: 755
	    - user: root
	    - group: root
	  cmd.run:
	    - name: cd /usr/local/src && tar xf haproxy-1.6.3.tar.gz && cd haproxy-1.6.3 && make TARGET=linux2628 && make install PREFIX=/usr/local/haproxy-1.6.3 && ln -s /usr/local/haproxy-1.6.3 /usr/local/haproxy
	    - unless: test -L /usr/local/haproxy
	    - require:
	      - pkg: make-pkg
	      - file: haproxy-install
	/etc/init.d/haproxy:
	  file.managed:
	    - source: salt://haproxy/files/haproxy.init 
	    - mode: 755
	    - user: root
	    - group: root
	    - require_in:
	      - file: haproxy-install
	
	net.ipv4.ip_nonlocal_bind:
	  sysctl.present:
	    - value: 1
	
	/etc/haproxy:
	  file.directory:
	    - user: root
	    - group: root
	    - mode: 755</pre>
	>文件分析：四部分构成：安装、配置文件部分、目录管理部分，网络设置 
3. 业务引用  
	>为了方便在生产中使用，将基础安装状态文件分类存放，环境如下:  
	>**cluster**：存放各种生产组合状态文件  
	>**modules**：存放基础状态文件
	1. cluster的创建
		<pre>mkdir cluster modules</pre>
	2. haproxy的对外配置文件
		<pre>cat files/haproxy-outside.cfg 
		global
		maxconn 100000
		chroot /usr/local/haproxy
		uid 99  
		gid 99 
		daemon
		nbproc 1 
		pidfile /usr/local/haproxy/logs/haproxy.pid 
		log 127.0.0.1 local3 info
		
		defaults
		option http-keep-alive
		maxconn 100000
		mode http
		timeout connect 5000ms
		timeout client  50000ms
		timeout server 50000ms
		
		listen stats
		mode http
		bind 0.0.0.0:8888
		stats enable
		stats uri     /haproxy-status 
		stats auth    haproxy:saltstack
		
		frontend frontend_www_example_com
		bind 10.0.0.180:80
		mode http
		option httplog
		log global
		    default_backend backend_www_example_com
		
		backend backend_www_example_com
		option forwardfor header X-REAL-IP
		option httpchk HEAD / HTTP/1.0
		balance source
		server web-node1  10.0.0.160:8080 check inter 2000 rise 30 fall 15
		server web-node2 10.0.0.171:8080 check inter 2000 rise 30 fall 15</pre>
	3. 服务管理文件
		<pre>cat haproxy-outside.sls 
		include:
		  - modules.haproxy.install
		
		haproxy-service:
		  file.managed:
		    - name: /etc/haproxy/haproxy.cfg
		    - source: salt://cluster/files/haproxy-outside.cfg
		    - user: root
		    - group: root
		    - mode: 644
		  service.running:
		    - name: haproxy
		    - enable: True
		    - reload: True
		    - require:
		      - cmd: haproxy-install
		    - watch:
		      - file: haproxy-service</pre>
	4. 编辑top文件，加入以下内容：
		<pre>prod:
		  'linux-node*':
		    - cluster.haproxy-outside</pre>
	5. 执行，测试
		<pre>salt '*' state.highstate test=True</pre>
		>在测试ok后，可正式安装。
		>另haproxy的访问地址：本机IP:8888/haproxy-status,用户名：haproxy；密码：saltstack

	
##问题汇总
1. `SaltRenderError: Jinja variable 'Zabbix_Server' is undefined`
	变量未定义或变量名不统一，检查后统一：
2. 格式问题，即缩进问题。 
	<pre>[ERROR   ] Data passed to highstate outputter is not a valid highstate return: {'linux-node2.oldboy.com': ['Rendering SLS \'base:init.zabbix_agent\' failed: while parsing a block collection\n  in "<unicode string>", line 5, column 5:\n        - name: /etc/zabbix/zabbix_agent ... \n        ^\nexpected <block end>, but found \'<block mapping start>\'\n  in "<unicode string>", line 9, column 6:\n         Zabbix_Server: 10.0.0.160\n         ^']}</pre>
3. `Error: rpmdb open failed`  
	重建rpmdb<pre>rm /var/lib/rpm/__db*
	rpm --rebuilddb
	yum update</pre>
