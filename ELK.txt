#ELKstack日志平台
#ELKstack快速搭建
- logstash(收集)
- elasticsearch(存数+搜索)
- kibana(展示)
- 安装环境需求
	- 两台机器，一台装elastisearch，另一台安装logstash和kibana
- Yum安装ELK
	- Elasticsearch部署
		>需JDK，检查是否有安装
		<pre>yum install -y java
		java -version</pre>
	- 安装
		<pre>rpm --import https://packages.elastic.co/GPG-KEY-elasticsearch 
		---添加elasticsearch仓库
		vim /etc/yum.repos.d/elasticsearch.repo
		[elasticsearch-2.x]
		name=Elasticsearch repository for 2.x packages
		baseurl=http://packages.elastic.co/elasticsearch/2.x/centos
		gpgcheck=1
		gpgkey=http://packages.elastic.co/GPG-KEY-elasticsearch
		enabled=1
		---安装
		yum install -y elasticsearch</pre>
- Yum安装logstash
	- 安装
		<pre>rpm --import https://packages.elastic.co/GPG-KEY-elasticsearch
		---添加logstash仓库
		vim /etc/yum.repos.d/logstash.repo
		[logstash-2.3]
		name=Logstash repository for 2.3.x packages
		baseurl=https://packages.elastic.co/logstash/2.3/centos
		gpgcheck=1
		gpgkey=https://packages.elastic.co/GPG-KEY-elasticsearch
		enabled=1
		---安装
		yum install -y logstash</pre>
- Yum安装Kibana
	- 安装
		<pre> rpm --import https://packages.elastic.co/GPG-KEY-elasticsearch
		---添加logstash仓库
		vim /etc/yum.repos.d/kibana.repo 
		[kibana-4.5]
		name=Kibana repository for 4.5.x packages
		baseurl=http://packages.elastic.co/kibana/4.5/centos
		gpgcheck=1
		gpgkey=http://packages.elastic.co/GPG-KEY-elasticsearch
		enabled=1
		---安装
		yum install -y kibana</pre>
- cobbler创建自己的ELK仓
	<pre> cobbler repo add --name=logstash-2.3 --mirror=http://packages.elastic.co/logstash/2.3/centos --arch=x86_64 --breed=yum
	cobbler repo add --name=elasticsearch2 --mirror=http://packages.elastic.co/ela ... entos --arch=x86_64 --breed=yum
	cobbler repo add --name=kibana4.5 --mirror=http://packages.elastic.co/kibana/4.5/centos --arch=x86_64 --breed=yum
	cobbler reposync</pre>
###Elasticsearch
- 配置文件
	<pre>grep '^[a-z]' elasticsearch.yml 
	cluster.name: myes   #集群名称
	node.name: cobbler   #节点名称
	path.data: /data/es-data    #数据保存位置
	path.logs: /var/logs/elasticsearch   #日志保存位置
	bootstrap.memory_lock: true   #锁定一直在内存，不进入交换分区
	network.host: 10.0.0.160   #主机IP地址
	http.port: 9200   #默认port
	无法连接的时候，使用：iptables -F</pre>
- 监控插件的安装
	<pre>/usr/share/elasticsearch/bin/plugin install marvel-agent</pre>
- head插件
	>集群管理的插件
	<pre>/usr/share/elasticsearch/bin/plugin install mobz/elasticsearch-head</pre>	
- bigdesk插件
	>性能监控，(es版本太高会有不支持的问题)
	<pre>/usr/share/elasticsearch/bin/plugin install lukas-vlcek/bigdesk</pre>
- kopf插件
	<pre>/usr/share/elasticsearch/bin/plugin install lmenezes/elasticsearch-kopf</pre>
- 访问方式
	http://IP:9200/_plugin/插件名称
- 测试
	>在http://IP:9200/_plugin/head上测试
	- 插入数据
		>/inde-demo/tes
		>{"user": "oldboy", "mesg": "hell world"}
	- 查看
		- 集群健康值（）
###ES集群
>ES集群没有主节点，任何一个节点都可以管理其他节点。
>节点2安装elasticsearch,IP:10.0.0.172,通讯方式可改外单播方式通讯。
>修改内容如下:  
>`discovery.zen.ping.unicast.hosts: ["10.0.0.160", "10.0.0.172"]`

- 监控状态
	<pre>curl -XGET "http://10.0.0.160:9200/_cluster/health?pretty=true"|grep status </pre>
	>其中黄色代表主分片没问题，但分片有问题
- cat /api
	[https://www.elastic.co/guide/en/elasticsearch/reference/current/cat.html](https://www.elastic.co/guide/en/elasticsearch/reference/current/cat.html)
- 环境搭建注意事项
	1. 硬件要求
	2. 系统要求：最大文件数（unlimt -SHn 65535）
###Logstash
- 角色：收集者  
- 主要概念：INPUT FILTER OUTPUT   
- 插件： stdin stdout
	>示例1：
	<pre># /opt/logstash/bin/logstash -e 'input { stdin{} } output{ stdout{} }'
	Settings: Default pipeline workers: 1
	Pipeline main started
	hell
	2016-09-24T11:45:04.483Z linux-node2 hell</pre>
	>示例2：
	<pre>/opt/logstash/bin/logstash -e 'input { stdin{} } output{ elasticsearch { hosts => ["10.0.0.172:9200"] index => "logstash-%{+YYYY.MM.dd}"} }'</pre>
	>示例3：屏幕和ES都有输出
	<pre>/opt/logstash/bin/logstash -e 'input { stdin{} } output{ stdout { code=>rubydebug }elasticsearch { hosts => ["10.0.0.172:9200"] index => "logstash-%{+YYYY.MM.dd}"} }'</pre>
- 收集日志方式
	- rsyslog -> ES
	- file -> ES
	- tcp -> ES
- logstash配置
	- 文件保存位置及写法
		<pre>cd /etc/logstash/conf.d/
		vi demo.conf
		input{
		  stdin{}
		}
		
		filter{
		}
		
		**output{
		  elasticsearch {
		    hosts => ["10.0.0.160"]
		    index => "logstash-%{+YYYY.MM.dd}"
		  }
		  stdout{
		    codec => rubydebug
		  }
		}</pre> 
		>说明：
		>1. 每行都是一个事件
		>2.input和output必不可少
		>3.基本流程：事件 -> input -> codec -> filter -> codec -> output
- input -> file 详解
	- format
		<pre>file {
		    path => ...
		}</pre>
	- path
		>Value type is array,The path(s) to the file(s) to use as an input. You can use filename patterns here, such as /var/log/*.log. If you use a pattern like /var/log/**/*.log, a recursive search of /var/log will be done for all *.log files. Paths must be absolute and cannot be relative.
	- sincedb
		>keeps track of the current position of monitored log files,The default will write sincedb files to <path.data>/plugins/inputs/file。作于相当于mysql主从中的pos点位置。
	- start_position
		>Choose where Logstash starts initially reading files: at the beginning or at the end. The default behavior treats files like live streams and thus starts at the end. If you have old data you want to import, set this to beginning.
	- type
		>Add a type field to all events handled by this input.Types are used mainly for filter activation.
###kibana
- 配置文件
	<pre>grep '^[a-z]' /opt/kibana/config/kibana.yml 
	elasticsearch.url: "http://10.0.0.160:9200"
	kibana.index: ".kibana"
	/etc/init.d/kibana start</pre>
	>访问地址：http://IP:5601
- 配置索引
	>1. 启动后需配置索引  
	>2. 刷新时间： quick、relative、absolute

###Logstash-input-if判断
- java日志示例：加入if判断
	<pre>cat /etc/logstash/conf.d/file.conf 
	input{
	  file{
	    path => ["/var/log/messages","/var/log/secure"]
	    type => "system-log"
	    start_position => "beginning"
	  }  
	  file{
	    path => ["/var/log/elasticsearch/myes.log"]
	    type => "es-log"
	  }
	}
	
	filter{
	}
	
	output{
	  if [type] == "system-log"{
	  elasticsearch{
	    hosts => ["10.0.0.160:9200"]
	    index => "system-log-%{+YYYY.MM}"
	    }
	  } 
	  if [type] == "es-log"{
	  elasticsearch{
	    hosts => ["10.0.0.160:9200"]
	    index => "es-log-%{+YYYY.MM}"
	    }
	  } 
	}
	#启动观察
	/opt/logstash/bin/logstash -f /etc/logstash/conf.d/file.conf </pre>
	- kibana增加索引，格式如下：**`[es-log-]YYYY.MM`**

- 将多行当做一个事件输出：multiline
	- 官方文档
	<pre>input {
		  stdin {
		    codec => multiline {
		      pattern => "pattern, a regexp"
		      negate => "true" or "false"
		      what => "previous" or "next"
		    }
		  }
		}</re>
	>pattern:合并要匹配的正则规则
	>negate：true是匹配上合并;false是没匹配上合并
	>what: previous 和 next,主要是和上面的行合并还是和下边的行合并
	- 示例：将java多行合并为一行
				<pre># cat file.conf      
		input{
		  file{
		    path => ["/var/log/messages","/var/log/secure"]
		    type => "system-log"
		    start_position => "beginning"
		  }  
		  file{
		    path => ["/var/log/elasticsearch/myes.log"]
		    type => "es-log"
		    start_position => "beginning"
		    codec => multiline{
		      pattern => "^\["
		      negate => true
		      what => "previous"
		   } 
		 }
		}
		
		filter{
		}
		
		output{
		  if [type] == "system-log"{
		  elasticsearch{
		    hosts => ["10.0.0.160:9200"]
		    index => "system-log-%{+YYYY.MM}"
		    }
		  } 
		  if [type] == "es-log"{
		  elasticsearch{
		    hosts => ["10.0.0.160:9200"]
		    index => "es-log-%{+YYYY.MM}"
		    }
		  } 
		}</pre>
###ELK 提取nginx 日志
>1. 将nginx访问日志修改成**`json`**格式,设计到的plugins为：**`codec`**
>2. 格式
	<pre>json {
  	}</pre>

- 模拟访问:
	`# ab -n 1000 -c 1 http://10.0.0.172/`	

- 整理nginx日志格式,修改为json格式
	<pre>log_format
	access_log_json  '{"user_ip":"$http_x_real_ip","lan_ip":"$remote_addr","log_time":"$time_is
	o8601","user_req":"$request","body_bytes_sent":"$body_bytes_sent","req_time":"$request_time","user":"$http_
	user_agent"}';               
    access_log  /var/log/nginx/access_json.log access_log_json;</pre>
- 其他方法
	- 文件直接读取：redis
	- python读取redis,写成json文件，然后写入ES
- logstash日志提取格式
	<pre># vi nginx.conf               
	input{
	  file {
	    path => "/var/log/nginx/access_json.log"
	    tupe => "nginx-access-log"
	    codec => "json"
	  }
	}
	
	filter{
	}
	
	output{
	  elasticsearch{
	    hosts => ["10.0.0.160:9200"]
	    index => "nginx-access-log-%{+YYYY.MM.DD}"
	    }
	}</pre> 